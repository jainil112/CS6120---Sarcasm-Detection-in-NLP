{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1c1FUIsTA69ED7sZEKUXyV9MhzV_5W2eF","authorship_tag":"ABX9TyO5387IgzDSeEKm58CSyimM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ec71374f6a054172a5f8ab61dc9be9db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a401dc116cde4aa6bd611b1f16bfe5d1","IPY_MODEL_8951b586a2fc441087ae8b9cda12f3af","IPY_MODEL_258d8fcad51f4772b3ca02d5a053e77d"],"layout":"IPY_MODEL_932a70bb9f2746239730e2b5ac3951f8"}},"a401dc116cde4aa6bd611b1f16bfe5d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab6e611ca87f48a9ab094759266fc7ab","placeholder":"​","style":"IPY_MODEL_4fdf15626b7a48b4b625ff1df82ebda7","value":"tokenizer_config.json: 100%"}},"8951b586a2fc441087ae8b9cda12f3af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07b89c6f117a40c38e9e05c7c97057eb","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44255c15fe8b4d06832a964ea96b69a4","value":48}},"258d8fcad51f4772b3ca02d5a053e77d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e56988b76a442d7a40615f81cff07e6","placeholder":"​","style":"IPY_MODEL_0f3fea1005034d2f80941045111eb212","value":" 48.0/48.0 [00:00&lt;00:00, 904B/s]"}},"932a70bb9f2746239730e2b5ac3951f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab6e611ca87f48a9ab094759266fc7ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdf15626b7a48b4b625ff1df82ebda7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07b89c6f117a40c38e9e05c7c97057eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44255c15fe8b4d06832a964ea96b69a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e56988b76a442d7a40615f81cff07e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f3fea1005034d2f80941045111eb212":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87cd6adb7a744766886d91fa912c81ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1c7da1853684e029cdcd654f8d1010f","IPY_MODEL_9702ed7ae1f04104bb4d7dab4814259e","IPY_MODEL_be06ff1ad6d647479daa42784b87cfc6"],"layout":"IPY_MODEL_95c1e70c0e6f4c399879315853614899"}},"e1c7da1853684e029cdcd654f8d1010f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03523c436f144fbd9f15de5b92bd5b53","placeholder":"​","style":"IPY_MODEL_1fcd992830a743e5a941f58f5cc0e7f6","value":"vocab.txt: 100%"}},"9702ed7ae1f04104bb4d7dab4814259e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a782bd8a5984dbda96eef9cf2a08274","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9eb3a139b1b84e7e9d9d6859cad4519d","value":231508}},"be06ff1ad6d647479daa42784b87cfc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c88a56da48ce4d61bbda8db8c5307522","placeholder":"​","style":"IPY_MODEL_fce304d075bd4608976cf63ad6c1c871","value":" 232k/232k [00:00&lt;00:00, 2.17MB/s]"}},"95c1e70c0e6f4c399879315853614899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03523c436f144fbd9f15de5b92bd5b53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fcd992830a743e5a941f58f5cc0e7f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a782bd8a5984dbda96eef9cf2a08274":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eb3a139b1b84e7e9d9d6859cad4519d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c88a56da48ce4d61bbda8db8c5307522":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fce304d075bd4608976cf63ad6c1c871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12b91d128a894805bcc2684301451f66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16142af1c32d45b4bd72b2fe9a32ca82","IPY_MODEL_0c48dbe58dbb42baaf96a6bbcbd01795","IPY_MODEL_1a836d66e6d24aa3a52fca7c030abd5b"],"layout":"IPY_MODEL_06f0ebe5562e4f04991fc85085239dda"}},"16142af1c32d45b4bd72b2fe9a32ca82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3d352a4ce7e4f6cbb5a230d1e52f7f9","placeholder":"​","style":"IPY_MODEL_d438ab4c3e8e4a8abc6ac3816113d8bc","value":"tokenizer.json: 100%"}},"0c48dbe58dbb42baaf96a6bbcbd01795":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d39e81014ecf4a0cba411a8aee2ec2fd","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_216ecafa2762435d903c6b478d60acb4","value":466062}},"1a836d66e6d24aa3a52fca7c030abd5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29617fdddf2f466b98ebba557ab9762d","placeholder":"​","style":"IPY_MODEL_9dd4e77d43b4411185d7897cba0e8f58","value":" 466k/466k [00:00&lt;00:00, 4.49MB/s]"}},"06f0ebe5562e4f04991fc85085239dda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3d352a4ce7e4f6cbb5a230d1e52f7f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d438ab4c3e8e4a8abc6ac3816113d8bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d39e81014ecf4a0cba411a8aee2ec2fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"216ecafa2762435d903c6b478d60acb4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29617fdddf2f466b98ebba557ab9762d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd4e77d43b4411185d7897cba0e8f58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b91a6b0615d4056bc2b7e72686cd36f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be78d8a683a34b5ea1091fa5eb5d36c3","IPY_MODEL_7d4b211d8ee14606a74537689232881e","IPY_MODEL_4ad764b902b54acb80d5dccd3e517bc7"],"layout":"IPY_MODEL_bf09a3c3f1cc4d1fba1821e10c29aa24"}},"be78d8a683a34b5ea1091fa5eb5d36c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb0cbef7b9974cc687faec7e675d884d","placeholder":"​","style":"IPY_MODEL_388b67ff69a34d38890a831ca328f006","value":"config.json: 100%"}},"7d4b211d8ee14606a74537689232881e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0d5d8404682448aab6fc20b5645059c","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4605b4f8c6e24bde8229c35fe4ce2fa0","value":483}},"4ad764b902b54acb80d5dccd3e517bc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57be01d10dbb4520b34466e3c9a6f84a","placeholder":"​","style":"IPY_MODEL_e868c8ae7c5045f5b52f911d6f24f295","value":" 483/483 [00:00&lt;00:00, 20.9kB/s]"}},"bf09a3c3f1cc4d1fba1821e10c29aa24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb0cbef7b9974cc687faec7e675d884d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"388b67ff69a34d38890a831ca328f006":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0d5d8404682448aab6fc20b5645059c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4605b4f8c6e24bde8229c35fe4ce2fa0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57be01d10dbb4520b34466e3c9a6f84a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e868c8ae7c5045f5b52f911d6f24f295":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecc5636163ea46028ff5079221f72c5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_172270be9cbb456baf4c70c53650c3cd","IPY_MODEL_d8e1053809ea4db2bbbab6ec3d71832e","IPY_MODEL_16beb36d4d6d4915a8166685213d1fe2"],"layout":"IPY_MODEL_e983b85a688d425e98b07ccd59fdc405"}},"172270be9cbb456baf4c70c53650c3cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f0ab78c7bf0461d83810c39a0b6e24d","placeholder":"​","style":"IPY_MODEL_611c40133da747c6b373dbe0bbc93a1f","value":"model.safetensors: 100%"}},"d8e1053809ea4db2bbbab6ec3d71832e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce2f6545b75d439d8f43ae87876ad3c0","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0be632831b6d4cd498c4168923cd1f32","value":267954768}},"16beb36d4d6d4915a8166685213d1fe2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea3e5a756e64be39ec1c41ddac2b21a","placeholder":"​","style":"IPY_MODEL_12425bdd57da4e27a19805e454797c8f","value":" 268M/268M [00:02&lt;00:00, 158MB/s]"}},"e983b85a688d425e98b07ccd59fdc405":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f0ab78c7bf0461d83810c39a0b6e24d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"611c40133da747c6b373dbe0bbc93a1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce2f6545b75d439d8f43ae87876ad3c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0be632831b6d4cd498c4168923cd1f32":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dea3e5a756e64be39ec1c41ddac2b21a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12425bdd57da4e27a19805e454797c8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Part 1: Imports and Initialization"],"metadata":{"id":"3XRzLdrrzaNn"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from scipy import sparse\n","import tensorflow as tf\n","from transformers import BertTokenizer, DistilBertTokenizer, TFDistilBertForSequenceClassification\n","from tabulate import tabulate\n","\n","# Download necessary NLTK data\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('wordnet', quiet=True)\n","\n","# Check GPU availability\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYhbCkGazY01","executionInfo":{"status":"ok","timestamp":1723487537655,"user_tz":240,"elapsed":13788,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}},"outputId":"abee02dd-e85f-4870-f4e3-f7f6c2cde722"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  0\n"]}]},{"cell_type":"markdown","source":["# Part 2: Load and Preprocess Data"],"metadata":{"id":"TRznKbXQzdvK"}},{"cell_type":"code","source":["# Step 1: Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/train-balanced-sarcasm.csv')\n","relevant_columns = ['label', 'comment', 'subreddit', 'parent_comment']\n","df = df[relevant_columns]\n","\n","# Step 3: Data ablation settings with stratified sampling\n","df_sample, _ = train_test_split(df, train_size=len(df) // 10000, stratify=df['label'], random_state=42)\n","df = df_sample.reset_index(drop=True)  # Reset index to avoid issues\n","\n","# Count and print the number of sarcastic (label=1) and non-sarcastic (label=0) entries after sampling\n","sarcastic_count = df['label'].sum()\n","non_sarcastic_count = len(df) - sarcastic_count\n","print(f\"\\nNumber of sarcastic entries: {sarcastic_count}\")\n","print(f\"Number of non-sarcastic entries: {non_sarcastic_count}\\n\")\n","\n","# Step 4: Preprocessing\n","\n","# def preprocess_text(text, lowercase=True, remove_stopwords=False, stemming=False, lemmatization=False, remove_punctuation=False):\n","#     if lowercase:\n","#         text = text.lower()\n","\n","#     if remove_punctuation:\n","#         text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","#     if remove_stopwords:\n","#         stop_words = set(stopwords.words('english'))\n","#         text = ' '.join([word for word in text.split() if word not in stop_words])\n","\n","#     if stemming:\n","#         stemmer = PorterStemmer()\n","#         text = ' '.join([stemmer.stem(word) for word in text.split()])\n","\n","#     if lemmatization:\n","#         lemmatizer = WordNetLemmatizer()\n","#         text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n","\n","#     return text\n","\n","# # Apply preprocessing to the 'comment' and 'parent_comment' columns\n","# df['processed_comment'] = df['comment'].apply(lambda x: preprocess_text(x,\n","#                                                               lowercase=True,\n","#                                                               remove_stopwords=True,\n","#                                                               stemming=False,\n","#                                                               lemmatization=True,\n","#                                                               remove_punctuation=True))\n","\n","# df['processed_parent_comment'] = df['parent_comment'].apply(lambda x: preprocess_text(x,\n","#                                                                             lowercase=True,\n","#                                                                             remove_stopwords=True,\n","#                                                                             stemming=False,\n","#                                                                             lemmatization=True,\n","#                                                                             remove_punctuation=True))\n","\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text, remove_stopwords=True, lemmatization=True):\n","    if pd.isna(text):\n","        return \"\"\n","    tokens = nltk.word_tokenize(str(text).lower())\n","    if remove_stopwords:\n","        tokens = [token for token in tokens if token not in stop_words]\n","    if lemmatization:\n","        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    return ' '.join(tokens)\n","\n","df['processed_comment'] = df['comment'].apply(preprocess_text)\n","df['processed_parent_comment'] = df['parent_comment'].apply(preprocess_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2tijKJYzYx8","executionInfo":{"status":"ok","timestamp":1723487551709,"user_tz":240,"elapsed":14067,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}},"outputId":"dce31876-f595-42e8-99b4-e8863e142301"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Number of sarcastic entries: 51\n","Number of non-sarcastic entries: 50\n","\n"]}]},{"cell_type":"markdown","source":["# Part 3: Feature Extraction and Train-Test Split"],"metadata":{"id":"iHKRGrO9zh9a"}},{"cell_type":"code","source":["# Step 5: Feature extraction\n","tfidf = TfidfVectorizer(max_features=5000)\n","tfidf_features = tfidf.fit_transform(df['processed_comment'])\n","tfidf_parent = TfidfVectorizer(max_features=5000)\n","tfidf_parent_features = tfidf_parent.fit_transform(df['processed_parent_comment'])\n","\n","df['comment_length'] = df['comment'].fillna('').apply(len)\n","df['parent_comment_length'] = df['parent_comment'].fillna('').apply(len)\n","\n","subreddit_dummies = pd.get_dummies(df['subreddit'], prefix='subreddit')\n","\n","numerical_features = df[['comment_length', 'parent_comment_length']].values\n","features = sparse.hstack([tfidf_features, tfidf_parent_features, numerical_features, subreddit_dummies])\n","\n","# Step 6: Train-test split (no stratify needed as it's already applied)\n","X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n","    features, df['label'], df.index, test_size=0.2, random_state=42\n",")\n","\n","# For BERT, extract the text using the original train indices\n","X_train_text = df['processed_comment'].iloc[train_indices]\n","X_test_text = df['processed_comment'].iloc[test_indices]\n"],"metadata":{"id":"rUUE3swEzYu5","executionInfo":{"status":"ok","timestamp":1723487551709,"user_tz":240,"elapsed":21,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Part 4: Model Training Functions"],"metadata":{"id":"DGyQaqHCzp0E"}},{"cell_type":"markdown","source":["Random Forest Model"],"metadata":{"id":"3Y1yHVEY1UBU"}},{"cell_type":"code","source":["# Random Forest\n","def train_random_forest(X_train, X_test, y_train, y_test):\n","    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","    rf_model.fit(X_train, y_train)\n","    return rf_model"],"metadata":{"id":"0ETcmtlizYsX","executionInfo":{"status":"ok","timestamp":1723487551710,"user_tz":240,"elapsed":20,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["SVM Model"],"metadata":{"id":"UF1R-n6G1bFd"}},{"cell_type":"code","source":["# SVM\n","def train_svm(X_train, X_test, y_train, y_test):\n","    svm_model = SVC(kernel='rbf', random_state=42, probability=True)\n","    svm_model.fit(X_train, y_train)\n","    return svm_model"],"metadata":{"id":"NB5GE7ux10kn","executionInfo":{"status":"ok","timestamp":1723487551710,"user_tz":240,"elapsed":20,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["BERT Model"],"metadata":{"id":"afA8mLjt1a7U"}},{"cell_type":"code","source":["# DistilBERT\n","def create_bert_model():\n","    bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-8)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","    return bert_model\n","\n","def prepare_bert_data(texts, tokenizer, max_length=128):\n","    encoded = tokenizer.batch_encode_plus(\n","        texts,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='tf'\n","    )\n","    return encoded['input_ids'], encoded['attention_mask']\n","\n","def train_bert(X_train_text, X_test_text, y_train, y_test):\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    X_train_ids, X_train_mask = prepare_bert_data(X_train_text, tokenizer)\n","\n","    # Create the BERT model\n","    bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","\n","    # Compile the model without explicitly specifying the optimizer\n","    bert_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","\n","    # Train the BERT model\n","    bert_model.fit(\n","        [X_train_ids, X_train_mask], y_train,\n","        epochs=3, batch_size=32, validation_split=0.1\n","    )\n","\n","    return bert_model, tokenizer"],"metadata":{"id":"D9cd2AXB1yxD","executionInfo":{"status":"ok","timestamp":1723487551710,"user_tz":240,"elapsed":19,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["LSTM Model"],"metadata":{"id":"7teUwDLY1axC"}},{"cell_type":"code","source":["# LSTM\n","def create_lstm_model(vocab_size, embedding_dim=100, max_length=100):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","        tf.keras.layers.LSTM(64),\n","        tf.keras.layers.Dense(32, activation='relu'),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def prepare_lstm_data(texts, tokenizer, max_length=100):\n","    sequences = tokenizer.texts_to_sequences(texts)\n","    padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')\n","    return padded\n","\n","def train_lstm(X_train_text, X_test_text, y_train, y_test):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts(X_train_text)\n","\n","    X_train_seq = prepare_lstm_data(X_train_text, tokenizer)\n","\n","    vocab_size = len(tokenizer.word_index) + 1\n","    lstm_model = create_lstm_model(vocab_size)\n","    lstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_split=0.1)\n","\n","    return lstm_model, tokenizer"],"metadata":{"id":"6sDk4pVl1u9K","executionInfo":{"status":"ok","timestamp":1723487551710,"user_tz":240,"elapsed":18,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Hybrid (BERT+LSTM) Model"],"metadata":{"id":"qzkvRrY71aq_"}},{"cell_type":"code","source":["# Hybrid model (DistilBERT + LSTM)\n","def create_hybrid_model(bert_model, lstm_model):\n","    bert_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='bert_input_ids')\n","    bert_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='bert_attention_mask')\n","    lstm_input = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name='lstm_input')\n","\n","    bert_output = bert_model([bert_input_ids, bert_attention_mask]).logits\n","    lstm_output = lstm_model(lstm_input)\n","\n","    combined = tf.keras.layers.concatenate([bert_output, lstm_output])\n","\n","    x = tf.keras.layers.Dense(64, activation='relu')(combined)\n","    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","    hybrid_model = tf.keras.Model(inputs=[bert_input_ids, bert_attention_mask, lstm_input], outputs=output)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-8)\n","    hybrid_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return hybrid_model\n","\n","def train_hybrid(bert_model, lstm_model, X_train_bert, X_train_lstm, y_train):\n","    hybrid_model = create_hybrid_model(bert_model, lstm_model)\n","    hybrid_model.fit(\n","        [X_train_bert[0], X_train_bert[1], X_train_lstm],\n","        y_train,\n","        epochs=3,\n","        batch_size=32,\n","        validation_split=0.1\n","    )\n","\n","    return hybrid_model"],"metadata":{"id":"CZIydMA01qh7","executionInfo":{"status":"ok","timestamp":1723487551959,"user_tz":240,"elapsed":266,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Part 5: Model Evaluation"],"metadata":{"id":"NJxCf16z16t6"}},{"cell_type":"code","source":["# Step 7: Model evaluation\n","def evaluate_model(y_true, y_pred, model_name):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    print(f\"{model_name} Results:\")\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1-score: {f1}\")\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","    print(\"-\" * 40)  # Separator for better output readability"],"metadata":{"id":"Srr6CDvI2PiN","executionInfo":{"status":"ok","timestamp":1723487551960,"user_tz":240,"elapsed":5,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Part 6: Cross-Validation"],"metadata":{"id":"oOTVjZfr194E"}},{"cell_type":"code","source":["# Step 8: Cross-validation for all models\n","def perform_cross_validation(model_name, model, X, y, tokenizer=None, cv=3):\n","    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n","    fold_scores = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(X)), y)):\n","        print(f\"Training fold {fold + 1}/{cv} for {model_name}...\")\n","\n","        if model_name in ['BERT', 'Hybrid', 'LSTM']:\n","            # Convert indices to lists to ensure compatibility with iloc\n","            X_train_text = X.iloc[train_idx].tolist()\n","            X_val_text = X.iloc[val_idx].tolist()\n","\n","            if model_name == 'BERT':\n","                X_train_ids, X_train_mask = prepare_bert_data(X_train_text, tokenizer)\n","                X_val_ids, X_val_mask = prepare_bert_data(X_val_text, tokenizer)\n","\n","                model.fit(\n","                    [X_train_ids, X_train_mask], y.iloc[train_idx],\n","                    validation_data=([X_val_ids, X_val_mask], y.iloc[val_idx]),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                val_pred_logits = model.predict([X_val_ids, X_val_mask]).logits\n","                val_preds = (tf.nn.sigmoid(val_pred_logits[:, 0]).numpy().flatten() > 0.5).astype(int)\n","\n","            elif model_name == 'LSTM':\n","                X_train_lstm = prepare_lstm_data(X_train_text, tokenizer)\n","                X_val_lstm = prepare_lstm_data(X_val_text, tokenizer)\n","\n","                model.fit(\n","                    X_train_lstm, y.iloc[train_idx],\n","                    validation_data=(X_val_lstm, y.iloc[val_idx]),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                val_preds = model.predict(X_val_lstm)\n","                val_preds = (val_preds > 0.5).astype(int).flatten()\n","\n","            elif model_name == 'Hybrid':\n","                X_train_ids, X_train_mask = prepare_bert_data(X_train_text, tokenizer)\n","                X_val_ids, X_val_mask = prepare_bert_data(X_val_text, tokenizer)\n","\n","                X_train_lstm = prepare_lstm_data(X_train_text, tokenizer)\n","                X_val_lstm = prepare_lstm_data(X_val_text, tokenizer)\n","\n","                model.fit(\n","                    [X_train_ids, X_train_mask, X_train_lstm], y.iloc[train_idx],\n","                    validation_data=([X_val_ids, X_val_mask, X_val_lstm], y.iloc[val_idx]),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                val_pred_logits = model.predict([X_val_ids, X_val_mask, X_val_lstm])\n","                val_preds = (tf.nn.sigmoid(val_pred_logits[:, 0]).numpy().flatten() > 0.5).astype(int)\n","\n","        else:\n","            # For non-BERT and non-LSTM models\n","            model.fit(X[train_idx], y.iloc[train_idx])\n","            val_preds = model.predict(X[val_idx])\n","\n","        accuracy = accuracy_score(y.iloc[val_idx], val_preds)\n","        precision, recall, f1, _ = precision_recall_fscore_support(y.iloc[val_idx], val_preds, average='binary')\n","        fold_scores.append((accuracy, precision, recall, f1))\n","        print(f\"Fold {fold + 1} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","        print(\"-\" * 40)\n","\n","    fold_scores = np.array(fold_scores)\n","    print(f\"\\n{model_name} Cross-Validation Results:\")\n","    print(f\"Mean Accuracy: {fold_scores[:, 0].mean()}\")\n","    print(f\"Mean Precision: {fold_scores[:, 1].mean()}\")\n","    print(f\"Mean Recall: {fold_scores[:, 2].mean()}\")\n","    print(f\"Mean F1-Score: {fold_scores[:, 3].mean()}\")\n","    print(\"-\" * 40)"],"metadata":{"id":"skAOye_G2Td9","executionInfo":{"status":"ok","timestamp":1723487551960,"user_tz":240,"elapsed":4,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Part 7: Ablation Studies"],"metadata":{"id":"lTtmrhvA1_l-"}},{"cell_type":"code","source":["# Step 9: Ablation studies for all models\n","def perform_ablation_studies(model_name, model, X, y, tokenizer=None):\n","    if model_name in ['BERT', 'Hybrid', 'LSTM']:\n","        feature_combinations = [\n","            ('Full Text', X)\n","        ]\n","    else:\n","        feature_combinations = [\n","            ('TF-IDF only', X[:, :5000]),\n","            ('TF-IDF + Numerical', X[:, :5002]),\n","            ('TF-IDF + Numerical + Subreddit', X)\n","        ]\n","\n","    for name, features in feature_combinations:\n","        print(f\"\\nAblation study: {name}\")\n","\n","        if model_name in ['BERT', 'Hybrid', 'LSTM']:\n","            X_train_text, X_test_text, y_train_fold, y_test_fold = train_test_split(features.tolist(), y, test_size=0.2, random_state=42)\n","\n","            if model_name == 'BERT':\n","                X_train_ids, X_train_mask = prepare_bert_data(X_train_text, tokenizer)\n","                X_test_ids, X_test_mask = prepare_bert_data(X_test_text, tokenizer)\n","\n","                model.fit(\n","                    [X_train_ids, X_train_mask], y_train_fold,\n","                    validation_data=([X_test_ids, X_test_mask], y_test_fold),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                val_pred_logits = model.predict([X_test_ids, X_test_mask]).logits\n","                y_pred = (tf.nn.sigmoid(val_pred_logits[:, 0]).numpy().flatten() > 0.5).astype(int)\n","            elif model_name == 'Hybrid':\n","                X_train_lstm = prepare_lstm_data(X_train_text, tokenizer)\n","                X_test_lstm = prepare_lstm_data(X_test_text, tokenizer)\n","\n","                X_train_ids, X_train_mask = prepare_bert_data(X_train_text, tokenizer)\n","                X_test_ids, X_test_mask = prepare_bert_data(X_test_text, tokenizer)\n","\n","                model.fit(\n","                    [X_train_ids, X_train_mask, X_train_lstm], y_train_fold,\n","                    validation_data=([X_test_ids, X_test_mask, X_test_lstm], y_test_fold),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                val_pred_logits = model.predict([X_test_ids, X_test_mask, X_test_lstm])\n","                y_pred = (tf.nn.sigmoid(val_pred_logits[:, 0]).numpy().flatten() > 0.5).astype(int)\n","            elif model_name == 'LSTM':\n","                X_train_lstm = prepare_lstm_data(X_train_text, tokenizer)\n","                X_test_lstm = prepare_lstm_data(X_test_text, tokenizer)\n","\n","                model.fit(\n","                    X_train_lstm, y_train_fold,\n","                    validation_data=(X_test_lstm, y_test_fold),\n","                    epochs=3,\n","                    batch_size=32\n","                )\n","                y_pred = model.predict(X_test_lstm)\n","                y_pred = (y_pred > 0.5).astype(int).flatten()\n","\n","        else:\n","            X_train, X_test, y_train_fold, y_test_fold = train_test_split(features, y, test_size=0.2, random_state=42)\n","            model.fit(X_train, y_train_fold)\n","            y_pred = model.predict(X_test)\n","\n","        accuracy = accuracy_score(y_test_fold, y_pred)\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_test_fold, y_pred, average='binary')\n","        print(f\"{model_name} ({name}) - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","        print(\"-\" * 40)"],"metadata":{"id":"Qr7ywYxnzzi5","executionInfo":{"status":"ok","timestamp":1723487551960,"user_tz":240,"elapsed":4,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Part 8: Model Comparison and Prediction"],"metadata":{"id":"0OwY5V1Dz1ss"}},{"cell_type":"code","source":["# Step 10: Model comparison\n","def compare_models(models, X_test, y_test, test_indices):\n","    results = []\n","    for name, model in models.items():\n","        if name in ['BERT', 'Hybrid']:\n","            if name == 'BERT':\n","                X_test_processed = prepare_bert_data(df.loc[test_indices, 'processed_comment'], bert_tokenizer)\n","                predictions = model.predict(X_test_processed, batch_size=32)\n","                logits = predictions.logits\n","            elif name == 'Hybrid':\n","                X_test_bert = prepare_bert_data(df.loc[test_indices, 'processed_comment'], bert_tokenizer)\n","                X_test_lstm = prepare_lstm_data(df.loc[test_indices, 'processed_comment'], lstm_tokenizer)\n","                predictions = model.predict([X_test_bert[0], X_test_bert[1], X_test_lstm], batch_size=32)\n","                logits = predictions\n","\n","            probabilities = tf.nn.sigmoid(logits[:, 0]).numpy().flatten()\n","            y_pred = (probabilities > 0.5).astype(int)\n","        elif name == 'LSTM':\n","            X_test_processed = prepare_lstm_data(df.loc[test_indices, 'processed_comment'], lstm_tokenizer)\n","            y_pred = model.predict(X_test_processed)\n","            y_pred = (y_pred > 0.5).astype(int).flatten()\n","        else:\n","            # Convert to dense if the model is SVM and the input is sparse\n","            if name == 'SVM' and sparse.issparse(X_test):\n","                X_test_dense = X_test.toarray()\n","                y_pred = model.predict(X_test_dense)\n","            else:\n","                y_pred = model.predict(X_test)\n","\n","        accuracy = accuracy_score(y_test, y_pred)\n","        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n","        results.append([name, accuracy, precision, recall, f1])\n","\n","    headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n","    print(tabulate(results, headers=headers, floatfmt=\".4f\"))\n","    print(\"-\" * 40)\n","\n","# Step 11: Prediction for new input\n","def predict_sarcasm(models, text):\n","    processed_text = preprocess_text(text)\n","\n","    results = []\n","    for name, model in models.items():\n","        if name == 'Random Forest' or name == 'SVM':\n","            tfidf_features = tfidf.transform([processed_text])\n","            tfidf_parent_features = tfidf_parent.transform([''])\n","            comment_length = len(text)\n","            parent_comment_length = 0\n","\n","            subreddit_dummies_empty = pd.DataFrame([np.zeros(subreddit_dummies.shape[1])], columns=subreddit_dummies.columns)\n","            numerical_features = np.array([[comment_length, parent_comment_length]])\n","\n","            features = sparse.hstack([tfidf_features, tfidf_parent_features, numerical_features, subreddit_dummies_empty])\n","\n","            # Convert to dense if the model is SVM and the input is sparse\n","            if name == 'SVM' and sparse.issparse(features):\n","                features = features.toarray()\n","\n","            prediction = model.predict(features)[0]\n","\n","        elif name == 'BERT':\n","            input_ids, attention_mask = prepare_bert_data([processed_text], bert_tokenizer)\n","            prediction_logits = model.predict([input_ids, attention_mask]).logits\n","            prediction_prob = tf.nn.sigmoid(prediction_logits[:, 0]).numpy().flatten()\n","            prediction = 1 if prediction_prob > 0.5 else 0\n","\n","        elif name == 'LSTM':\n","            sequence = prepare_lstm_data([processed_text], lstm_tokenizer)\n","            prediction_prob = model.predict(sequence)[0][0]\n","            prediction = 1 if prediction_prob > 0.5 else 0\n","\n","        elif name == 'Hybrid':\n","            input_ids, attention_mask = prepare_bert_data([processed_text], bert_tokenizer)\n","            lstm_input = prepare_lstm_data([processed_text], lstm_tokenizer)\n","            prediction_logits = model.predict([input_ids, attention_mask, lstm_input])\n","            prediction_prob = tf.nn.sigmoid(prediction_logits[:, 0]).numpy().flatten()\n","            prediction = 1 if prediction_prob > 0.5 else 0\n","\n","        results.append([name, \"Sarcastic\" if prediction == 1 else \"Not sarcastic\"])\n","\n","    headers = [\"Model\", \"Prediction\"]\n","    print(tabulate(results, headers=headers))\n","    print(\"-\" * 40)\n"],"metadata":{"id":"V-avp2Ccz54_","executionInfo":{"status":"ok","timestamp":1723487551961,"user_tz":240,"elapsed":4,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Part 9: Main Execution and Interactive Prediction"],"metadata":{"id":"ZyS2aLpd0Ds4"}},{"cell_type":"code","source":["# Main execution\n","if __name__ == \"__main__\":\n","    # Train Random Forest\n","    print(\"Training Random Forest...\")\n","    rf_model = train_random_forest(X_train, X_test, y_train, y_test)\n","    rf_pred = rf_model.predict(X_test)\n","    evaluate_model(y_test, rf_pred, \"Random Forest\")\n","    perform_cross_validation(\"Random Forest\", rf_model, features.toarray(), df['label'])\n","    perform_ablation_studies(\"Random Forest\", rf_model, features.toarray(), df['label'])\n","\n","    # Train SVM\n","    print(\"\\nTraining SVM...\")\n","    svm_model = train_svm(X_train, X_test, y_train, y_test)\n","    svm_pred = svm_model.predict(X_test)\n","    evaluate_model(y_test, svm_pred, \"SVM\")\n","\n","    # Train BERT\n","    print(\"\\nTraining BERT...\")\n","    bert_model, bert_tokenizer = train_bert(X_train_text, X_test_text, y_train, y_test)\n","    X_test_bert = prepare_bert_data(X_test_text, bert_tokenizer)\n","    bert_pred = bert_model.predict([X_test_bert[0], X_test_bert[1]])\n","    bert_pred = np.argmax(bert_pred.logits, axis=1)\n","    evaluate_model(y_test, bert_pred, \"BERT\")\n","\n","    # Initialize LSTM tokenizer before using it\n","    lstm_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    lstm_tokenizer.fit_on_texts(X_train_text)  # Assuming X_train_text is the training text data\n","\n","    # Train LSTM\n","    print(\"\\nTraining LSTM...\")\n","    lstm_model, lstm_tokenizer = train_lstm(X_train_text, X_test_text, y_train, y_test)\n","    X_test_lstm = prepare_lstm_data(X_test_text, lstm_tokenizer)\n","    lstm_pred = lstm_model.predict(X_test_lstm)\n","    lstm_pred = (lstm_pred > 0.5).astype(int).flatten()\n","    evaluate_model(y_test, lstm_pred, \"LSTM\")\n","    perform_cross_validation(\"LSTM\", lstm_model, X_train_text, y_train, tokenizer=lstm_tokenizer)\n","    perform_ablation_studies(\"LSTM\", lstm_model, X_train_text, y_train, tokenizer=lstm_tokenizer)\n","\n","    # Train Hybrid (BERT + LSTM)\n","    print(\"\\nTraining Hybrid model...\")\n","    X_train_bert = prepare_bert_data(X_train_text, bert_tokenizer)\n","    X_train_lstm = prepare_lstm_data(X_train_text, lstm_tokenizer)\n","    hybrid_model = train_hybrid(bert_model, lstm_model, X_train_bert, X_train_lstm, y_train)\n","    X_test_bert = prepare_bert_data(X_test_text, bert_tokenizer)\n","    X_test_lstm = prepare_lstm_data(X_test_text, lstm_tokenizer)\n","    hybrid_pred = hybrid_model.predict([X_test_bert[0], X_test_bert[1], X_test_lstm])\n","    hybrid_pred = (hybrid_pred > 0.5).astype(int).flatten()\n","    evaluate_model(y_test, hybrid_pred, \"Hybrid (BERT + LSTM)\")\n","\n","    # Store all trained models in a dictionary\n","    models = {\n","        \"Random Forest\": rf_model,\n","        \"SVM\": svm_model,\n","        \"BERT\": bert_model,\n","        \"LSTM\": lstm_model,\n","        \"Hybrid\": hybrid_model\n","    }\n","\n","    # Compare all models\n","    print(\"\\nModel Comparison:\")\n","    compare_models(models, X_test, y_test, test_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ec71374f6a054172a5f8ab61dc9be9db","a401dc116cde4aa6bd611b1f16bfe5d1","8951b586a2fc441087ae8b9cda12f3af","258d8fcad51f4772b3ca02d5a053e77d","932a70bb9f2746239730e2b5ac3951f8","ab6e611ca87f48a9ab094759266fc7ab","4fdf15626b7a48b4b625ff1df82ebda7","07b89c6f117a40c38e9e05c7c97057eb","44255c15fe8b4d06832a964ea96b69a4","1e56988b76a442d7a40615f81cff07e6","0f3fea1005034d2f80941045111eb212","87cd6adb7a744766886d91fa912c81ef","e1c7da1853684e029cdcd654f8d1010f","9702ed7ae1f04104bb4d7dab4814259e","be06ff1ad6d647479daa42784b87cfc6","95c1e70c0e6f4c399879315853614899","03523c436f144fbd9f15de5b92bd5b53","1fcd992830a743e5a941f58f5cc0e7f6","5a782bd8a5984dbda96eef9cf2a08274","9eb3a139b1b84e7e9d9d6859cad4519d","c88a56da48ce4d61bbda8db8c5307522","fce304d075bd4608976cf63ad6c1c871","12b91d128a894805bcc2684301451f66","16142af1c32d45b4bd72b2fe9a32ca82","0c48dbe58dbb42baaf96a6bbcbd01795","1a836d66e6d24aa3a52fca7c030abd5b","06f0ebe5562e4f04991fc85085239dda","a3d352a4ce7e4f6cbb5a230d1e52f7f9","d438ab4c3e8e4a8abc6ac3816113d8bc","d39e81014ecf4a0cba411a8aee2ec2fd","216ecafa2762435d903c6b478d60acb4","29617fdddf2f466b98ebba557ab9762d","9dd4e77d43b4411185d7897cba0e8f58","4b91a6b0615d4056bc2b7e72686cd36f","be78d8a683a34b5ea1091fa5eb5d36c3","7d4b211d8ee14606a74537689232881e","4ad764b902b54acb80d5dccd3e517bc7","bf09a3c3f1cc4d1fba1821e10c29aa24","bb0cbef7b9974cc687faec7e675d884d","388b67ff69a34d38890a831ca328f006","a0d5d8404682448aab6fc20b5645059c","4605b4f8c6e24bde8229c35fe4ce2fa0","57be01d10dbb4520b34466e3c9a6f84a","e868c8ae7c5045f5b52f911d6f24f295","ecc5636163ea46028ff5079221f72c5e","172270be9cbb456baf4c70c53650c3cd","d8e1053809ea4db2bbbab6ec3d71832e","16beb36d4d6d4915a8166685213d1fe2","e983b85a688d425e98b07ccd59fdc405","9f0ab78c7bf0461d83810c39a0b6e24d","611c40133da747c6b373dbe0bbc93a1f","ce2f6545b75d439d8f43ae87876ad3c0","0be632831b6d4cd498c4168923cd1f32","dea3e5a756e64be39ec1c41ddac2b21a","12425bdd57da4e27a19805e454797c8f"]},"id":"yhyoqotp0GP5","executionInfo":{"status":"ok","timestamp":1723487802731,"user_tz":240,"elapsed":250774,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}},"outputId":"dfdc8270-6b47-44b1-acf3-bfcb2f72729e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Random Forest...\n","Random Forest Results:\n","Accuracy: 0.47619047619047616\n","Precision: 0.45\n","Recall: 1.0\n","F1-score: 0.6206896551724138\n","Confusion Matrix:\n","[[ 1 11]\n"," [ 0  9]]\n","----------------------------------------\n","Training fold 1/2 for Random Forest...\n","Fold 1 - Accuracy: 0.43137254901960786, Precision: 0.46153846153846156, Recall: 0.6923076923076923, F1: 0.553846153846154\n","----------------------------------------\n","Training fold 2/2 for Random Forest...\n","Fold 2 - Accuracy: 0.44, Precision: 0.46511627906976744, Recall: 0.8, F1: 0.5882352941176471\n","----------------------------------------\n","\n","Random Forest Cross-Validation Results:\n","Mean Accuracy: 0.4356862745098039\n","Mean Precision: 0.4633273703041145\n","Mean Recall: 0.7461538461538462\n","Mean F1-Score: 0.5710407239819005\n","----------------------------------------\n","\n","Ablation study: TF-IDF only\n","Random Forest (TF-IDF only) - Accuracy: 0.47619047619047616, Precision: 0.45, Recall: 1.0, F1: 0.6206896551724138\n","----------------------------------------\n","\n","Ablation study: TF-IDF + Numerical\n","Random Forest (TF-IDF + Numerical) - Accuracy: 0.47619047619047616, Precision: 0.45, Recall: 1.0, F1: 0.6206896551724138\n","----------------------------------------\n","\n","Ablation study: TF-IDF + Numerical + Subreddit\n","Random Forest (TF-IDF + Numerical + Subreddit) - Accuracy: 0.47619047619047616, Precision: 0.45, Recall: 1.0, F1: 0.6206896551724138\n","----------------------------------------\n","\n","Training SVM...\n","SVM Results:\n","Accuracy: 0.6190476190476191\n","Precision: 0.5294117647058824\n","Recall: 1.0\n","F1-score: 0.6923076923076924\n","Confusion Matrix:\n","[[4 8]\n"," [0 9]]\n","----------------------------------------\n","\n","Training BERT...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec71374f6a054172a5f8ab61dc9be9db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87cd6adb7a744766886d91fa912c81ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b91d128a894805bcc2684301451f66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b91a6b0615d4056bc2b7e72686cd36f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc5636163ea46028ff5079221f72c5e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["9/9 [==============================] - 86s 8s/step - loss: 0.7919 - accuracy: 0.5278 - val_loss: 0.9260 - val_accuracy: 0.5000\n","1/1 [==============================] - 8s 8s/step\n","BERT Results:\n","Accuracy: 0.42857142857142855\n","Precision: 0.42857142857142855\n","Recall: 1.0\n","F1-score: 0.6\n","Confusion Matrix:\n","[[ 0 12]\n"," [ 0  9]]\n","----------------------------------------\n","\n","Training LSTM...\n","Epoch 1/5\n","3/3 [==============================] - 3s 362ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n","Epoch 2/5\n","3/3 [==============================] - 0s 69ms/step - loss: 0.6928 - accuracy: 0.5278 - val_loss: 0.6933 - val_accuracy: 0.5000\n","Epoch 3/5\n","3/3 [==============================] - 0s 71ms/step - loss: 0.6923 - accuracy: 0.5278 - val_loss: 0.6936 - val_accuracy: 0.5000\n","Epoch 4/5\n","3/3 [==============================] - 0s 78ms/step - loss: 0.6919 - accuracy: 0.5278 - val_loss: 0.6938 - val_accuracy: 0.5000\n","Epoch 5/5\n","3/3 [==============================] - 0s 72ms/step - loss: 0.6921 - accuracy: 0.5278 - val_loss: 0.6941 - val_accuracy: 0.5000\n","1/1 [==============================] - 0s 458ms/step\n","LSTM Results:\n","Accuracy: 0.42857142857142855\n","Precision: 0.42857142857142855\n","Recall: 1.0\n","F1-score: 0.6\n","Confusion Matrix:\n","[[ 0 12]\n"," [ 0  9]]\n","----------------------------------------\n","Training fold 1/2 for LSTM...\n","Epoch 1/3\n","2/2 [==============================] - 0s 169ms/step - loss: 0.6919 - accuracy: 0.5250 - val_loss: 0.6919 - val_accuracy: 0.5250\n","Epoch 2/3\n","2/2 [==============================] - 0s 119ms/step - loss: 0.6916 - accuracy: 0.5250 - val_loss: 0.6920 - val_accuracy: 0.5250\n","Epoch 3/3\n","2/2 [==============================] - 0s 99ms/step - loss: 0.6913 - accuracy: 0.5250 - val_loss: 0.6922 - val_accuracy: 0.5250\n","2/2 [==============================] - 0s 14ms/step\n","Fold 1 - Accuracy: 0.525, Precision: 0.525, Recall: 1.0, F1: 0.6885245901639345\n","----------------------------------------\n","Training fold 2/2 for LSTM...\n","Epoch 1/3\n","2/2 [==============================] - 0s 191ms/step - loss: 0.6917 - accuracy: 0.5250 - val_loss: 0.6928 - val_accuracy: 0.5250\n","Epoch 2/3\n","2/2 [==============================] - 0s 131ms/step - loss: 0.6925 - accuracy: 0.5250 - val_loss: 0.6937 - val_accuracy: 0.5250\n","Epoch 3/3\n","2/2 [==============================] - 0s 119ms/step - loss: 0.6945 - accuracy: 0.5250 - val_loss: 0.6945 - val_accuracy: 0.5250\n","2/2 [==============================] - 0s 14ms/step\n","Fold 2 - Accuracy: 0.525, Precision: 0.525, Recall: 1.0, F1: 0.6885245901639345\n","----------------------------------------\n","\n","LSTM Cross-Validation Results:\n","Mean Accuracy: 0.525\n","Mean Precision: 0.525\n","Mean Recall: 1.0\n","Mean F1-Score: 0.6885245901639345\n","----------------------------------------\n","\n","Ablation study: Full Text\n","Epoch 1/3\n","2/2 [==============================] - 0s 152ms/step - loss: 0.6935 - accuracy: 0.5312 - val_loss: 0.7011 - val_accuracy: 0.5000\n","Epoch 2/3\n","2/2 [==============================] - 0s 109ms/step - loss: 0.6932 - accuracy: 0.5312 - val_loss: 0.7004 - val_accuracy: 0.5000\n","Epoch 3/3\n","2/2 [==============================] - 0s 156ms/step - loss: 0.6928 - accuracy: 0.5312 - val_loss: 0.6996 - val_accuracy: 0.5000\n","1/1 [==============================] - 0s 54ms/step\n","LSTM (Full Text) - Accuracy: 0.5, Precision: 0.5, Recall: 1.0, F1: 0.6666666666666666\n","----------------------------------------\n","\n","Training Hybrid model...\n","9/9 [==============================] - 85s 8s/step - loss: 0.6960 - accuracy: 0.4722 - val_loss: 0.6936 - val_accuracy: 0.5000\n","1/1 [==============================] - 7s 7s/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Hybrid (BERT + LSTM) Results:\n","Accuracy: 0.5714285714285714\n","Precision: 0.0\n","Recall: 0.0\n","F1-score: 0.0\n","Confusion Matrix:\n","[[12  0]\n"," [ 9  0]]\n","----------------------------------------\n","\n","Model Comparison:\n","1/1 [==============================] - 6s 6s/step\n","1/1 [==============================] - 0s 33ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 7s 7s/step\n","Model            Accuracy    Precision    Recall    F1-Score\n","-------------  ----------  -----------  --------  ----------\n","Random Forest      0.4762       0.4500    1.0000      0.6207\n","SVM                0.6190       0.5294    1.0000      0.6923\n","BERT               0.5714       0.0000    0.0000      0.0000\n","LSTM               0.4286       0.4286    1.0000      0.6000\n","Hybrid             0.4286       0.4286    1.0000      0.6000\n","----------------------------------------\n"]}]},{"cell_type":"markdown","source":["User Interactive Prediction"],"metadata":{"id":"nfGnRzOu0YY4"}},{"cell_type":"code","source":["    # Example of predicting sarcasm for a new input\n","    print(\"\\nSarcasm Prediction for New Input:\")\n","    new_text = \"I absolutely love waiting in long lines at the DMV. It's the highlight of my day!\"\n","    predict_sarcasm(models, new_text)\n","\n","    # Interactive prediction\n","    while True:\n","        user_input = input(\"\\nEnter a comment to check for sarcasm (or 'quit' to exit): \")\n","        if user_input.lower() == 'quit':\n","            break\n","        predict_sarcasm(models, user_input)\n","\n","print(\"Program completed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvUSN73q0XGE","executionInfo":{"status":"ok","timestamp":1723487855586,"user_tz":240,"elapsed":52861,"user":{"displayName":"GPT Plus","userId":"05196704298814320730"}},"outputId":"18f45437-d17c-438f-bfba-bc841b07ecc7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sarcasm Prediction for New Input:\n","1/1 [==============================] - 1s 902ms/step\n","1/1 [==============================] - 0s 95ms/step\n","1/1 [==============================] - 1s 853ms/step\n","Model          Prediction\n","-------------  -------------\n","Random Forest  Sarcastic\n","SVM            Sarcastic\n","BERT           Not sarcastic\n","LSTM           Sarcastic\n","Hybrid         Sarcastic\n","----------------------------------------\n","\n","Enter a comment to check for sarcasm (or 'quit' to exit): We are working on this feature, it can not predict accurately. Thanks.\n","1/1 [==============================] - 0s 350ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 349ms/step\n","Model          Prediction\n","-------------  -------------\n","Random Forest  Sarcastic\n","SVM            Sarcastic\n","BERT           Not sarcastic\n","LSTM           Sarcastic\n","Hybrid         Sarcastic\n","----------------------------------------\n","\n","Enter a comment to check for sarcasm (or 'quit' to exit): quit\n","Program completed.\n"]}]}]}